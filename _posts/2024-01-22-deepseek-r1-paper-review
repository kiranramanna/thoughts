---
layout: post
title:  "Deepseek R1 Paper review"
date:   2024-01-22 00:42:45 +0900
categories: LLM, deepseek
---

**DeepSeek-R1: Incentivizing Reasoning Capability in Large Language Models**  

Ever wondered how large language models (LLMs) can learn to *reason*â€”not just predict the next word? The DeepSeek-R1 paper explores how *reinforcement learning* (RL) techniques can push LLMs toward more advanced reasoning, using an innovative approach called **GRPO (Guided Reward Policy Optimization)**. Here are the key insights:

- âš™ï¸ **LLM as a Policy:**  
  Instead of just predicting the next word, think of the language model as an *agent* making decisions (tokens) in a *state* (the prompt). Reinforcement learning frameworks help optimize these decisions for better long-term outcomes.

- ğŸ¯ **Rule-Based Rewards:**  
  Unlike typical RL from human feedback (where a learned reward model scores answers), DeepSeek-R1 uses *rule-based* metricsâ€”like checking if code compiles or if a math solution is correct. This automatically provides clear signals about *good* vs. *bad* outputs.

- â™»ï¸ **Off-Policy & Clipping:**  
  By sampling answers once and reusing that data many times (off-policy training), the model trains efficiently. Clipping in GRPO prevents â€œreward hacking,â€ so the model can improve without straying too far from its original capabilities.

- ğŸ§  **Emergent Chain-of-Thought:**  
  With the right incentives (i.e., achieving the correct solution), the model *naturally* learns to break complex tasks into steps. Thereâ€™s no explicit â€œteach step-by-step reasoningâ€ instructionâ€”the RL signal guides the model to develop it on its own.

- ğŸ’¡ **Distillation for Efficiency:**  
  To make smaller models more capable, the â€œbig brotherâ€ modelâ€™s outputsâ€”*including its token-level probabilities*â€”are used to train the smaller one. This *knowledge distillation* accelerates learning and preserves essential reasoning skills.

- âš–ï¸ **Balancing Change & Stability:**  
  Reinforcement learning updates must be carefully tuned. A divergence penalty ensures the model doesnâ€™t discard all prior knowledge (and just generate â€œthank youâ€ repeatedly to boost a politeness reward). Instead, it refines *useful* behaviors without losing core language abilities.

- ğŸš€ **Practical Implications:**  
  DeepSeek-R1 shows how *pure RL*â€”with minimal direct instructionâ€”can lead to *powerful, emergent reasoning*. This has massive potential in coding assistants, math solvers, and beyond, where step-by-step problem-solving is crucial.

  
**Why It Matters**  
As businesses explore AI-driven solutions, harnessing *reasoning*â€”not just pattern matchingâ€”can unlock more reliable, creative, and higher-level performance. DeepSeek-R1 demonstrates a *self-improvement loop* where models learn to think through problems more systematically.  

**Your Turn**  
Whatâ€™s one area in your work or industry that could benefit from more *step-by-step, AI-driven reasoning*? Share your thoughts or experiences in the comments. Letâ€™s continue the conversation on how RL can help LLMs evolve beyond â€œtext predictionâ€ into true problem solvers.  